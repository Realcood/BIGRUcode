import cv2
import torch
import numpy as np
import mediapipe as mp
from model import FallBiGRUAttentionNet
from PIL import ImageFont, ImageDraw, Image
import os

# âœ… ì‚¬ìš©í•  ê´€ì ˆ index
SELECTED_IDX = [0, 10, 15, 16, 23, 24]
PART_MAP = {0: "ë¨¸ë¦¬", 1: "ì†ëª©", 2: "ê³¨ë°˜", 3: "ê¸°íƒ€"}

# âœ… í°íŠ¸ ì„¤ì •
font_path = "C:/Windows/Fonts/malgun.ttf"
if os.path.exists(font_path):
    font = ImageFont.truetype(font_path, 32)
else:
    print("âš ï¸ í•œê¸€ í°íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.")
    font = ImageFont.load_default()

# âœ… ëª¨ë¸ ë¡œë“œ
model = FallBiGRUAttentionNet(input_dim=24, hidden_dim=128, num_layers=2)
model.load_state_dict(torch.load("fall_bigru_model.pth", map_location="cpu"))
model.eval()

# âœ… MediaPipe ì´ˆê¸°í™”
pose = mp.solutions.pose.Pose()
mp_drawing = mp.solutions.drawing_utils

# âœ… ì¢…ë£Œ ì‹ í˜¸ ë³€ìˆ˜
exit_flag = [False]  # ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë‚´ë¶€ í•¨ìˆ˜ì—ì„œë„ ìˆ˜ì • ê°€ëŠ¥

# âœ… ë§ˆìš°ìŠ¤ ì½œë°± í•¨ìˆ˜ ì •ì˜
def mouse_callback(event, x, y, flags, param):
    if event == cv2.EVENT_RBUTTONDOWN:  # ë§ˆìš°ìŠ¤ ìš°í´ë¦­
        print("ğŸ–±ï¸ ë§ˆìš°ìŠ¤ ìš°í´ë¦­ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit_flag[0] = True

# âœ… ì°½ ì´ˆê¸°í™” ë° ë§ˆìš°ìŠ¤ ì½œë°± ë“±ë¡
cv2.namedWindow("ğŸŸ¢ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ (í•œê¸€ í‘œì‹œ OK)")
cv2.setMouseCallback("ğŸŸ¢ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ (í•œê¸€ í‘œì‹œ OK)", mouse_callback)

# âœ… ì¹´ë©”ë¼ ì‹œì‘
cap = cv2.VideoCapture(0)
sequence = []
prev_zs = None
last_label = "ë¶„ì„ ì¤‘..."
last_part = "-"

print("ğŸ“¸ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (q ë˜ëŠ” ìš°í´ë¦­ìœ¼ë¡œ ì¢…ë£Œ)")

while True:
    ret, frame = cap.read()
    if not ret:
        print("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = pose.process(rgb)

    if result.pose_landmarks:
        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)
        keypoints = []
        current_zs = []

        for idx in SELECTED_IDX:
            lm = result.pose_landmarks.landmark[idx]
            current_zs.append(lm.z)

        for i, idx in enumerate(SELECTED_IDX):
            lm = result.pose_landmarks.landmark[idx]
            z_now = lm.z
            z_prev = prev_zs[i] if prev_zs else z_now
            z_speed = z_now - z_prev
            keypoints.extend([lm.x, lm.y, lm.z, z_speed])
        prev_zs = current_zs

        sequence.append(keypoints)
        if len(sequence) > 30:
            sequence.pop(0)

        if len(sequence) == 30:
            input_tensor = torch.tensor([sequence], dtype=torch.float32)
            with torch.no_grad():
                fall_out, part_out = model(input_tensor)
                fall_pred = torch.argmax(fall_out, 1).item()
                part_pred = torch.argmax(part_out, 1).item()

            last_label = "ğŸ’¥ ë‚™ìƒ ë°œìƒ!" if fall_pred == 1 else "ì •ìƒì…ë‹ˆë‹¤"
            last_part = PART_MAP[part_pred]

    frame_pil = Image.fromarray(frame)
    draw = ImageDraw.Draw(frame_pil)
    text = f"ë‚™ìƒ ë°œìƒ : {last_part}" if last_label.startswith("ğŸ’¥") else "ì •ìƒì…ë‹ˆë‹¤"
    color = (0, 0, 255) if last_label.startswith("ğŸ’¥") else (0, 255, 0)

    text_size = draw.textbbox((0, 0), text, font=font)
    box_w = text_size[2] - text_size[0]
    box_h = text_size[3] - text_size[1]
    draw.rectangle([(25, 25), (25 + box_w + 10, 25 + box_h + 10)], fill=(0, 0, 0, 180))
    draw.text((30, 30), text, font=font, fill=color)

    frame = np.array(frame_pil)
    cv2.imshow("ğŸŸ¢ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ (í•œê¸€ í‘œì‹œ OK)", frame)

    if cv2.waitKey(1) & 0xFF == ord('q') or exit_flag[0]:
        print("ğŸ›‘ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

cap.release()
cv2.destroyAllWindows()
